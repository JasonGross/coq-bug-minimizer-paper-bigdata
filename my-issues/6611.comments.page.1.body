It looks like more and more time is passed in the OCaml compiler function `Lambda.free`.
But wait, your terms are growing bigger, so it's expected that compilation time increases. And resetting it doesn't change anything for me. Am I missing something?
@ppedrot Note that at each successive iteration, the term that I pass has (at least) one more binder / lambda term.  Does the additional time spent in `Lambda.free` come from this, or is it the thing that goes down when you restart Coq?
@ppedrot You can't reproduce the effect that I saw if you run the two files in sequence?  The end of the first file has:
```
native timing n= 785
Tactic call ran for 1.094 secs (0.039u,0.004s) (success)
native timing n= 790
Tactic call ran for 1.083 secs (0.035u,0.004s) (success)
native timing n= 795
Tactic call ran for 1.114 secs (0.044u,0.004s) (success)
native timing n= 800
Tactic call ran for 1.104 secs (0.039u,0.004s) (success)
```
and the beginning of the second is
```
native timing n= 805
Tactic call ran for 1.082 secs (0.012u,0.008s) (success)
native timing n= 810
Tactic call ran for 1.084 secs (0.012u,0.008s) (success)
native timing n= 815
Tactic call ran for 1.094 secs (0.016u,0.003s) (success)
native timing n= 820
Tactic call ran for 1.096 secs (0.02u,0.s) (success)
```
Note that the user time is about 4x faster between the end of the first and the beginning of the second, and the real time is consistently about 2% faster.
@ppedrot Here is one that demonstrates the issue with decreasing term size:
```coq
Fixpoint expr_of_nat {var} (n : nat) : @expr var
  := match n with
     | O => NatO
     | S n => NatS (expr_of_nat n)
     end.

Definition tests3
  := List.map (fun n => (fun var => @big' var (expr_of_nat n) (count_of_nat 800)))
              (List.rev (List.seq 1 600)).

Ltac foreach1 tac ls :=
  lazymatch ls with
  | nil => idtac
  | cons ?term ?ls
    => tac term; foreach1 tac ls
  end.

Ltac test_one_native3 term :=
  time try (let k := (eval native_compute in term) in fail);
  idtac.

Ltac test_native_3 :=
  let tests := tests3 in
  let ls := (eval lazy in tests) in
  let ls := (eval vm_compute in tests) in
  foreach1 test_one_native3 ls.
```

It gives this graph of user time vs iteration:
![user time vs iteration](https://user-images.githubusercontent.com/396076/35128104-dfc42a6e-fc82-11e7-9f82-413401ef0d99.png)

There is a slow but definite increase in user-time, even as term size decreases (slightly).

By contrast, the real time is kind-of all over the place:
![real time vs iteration](https://user-images.githubusercontent.com/396076/35128216-64875e2e-fc83-11e7-9c1b-e573c98be972.png)

@JasonGross did you manage to reach significant user time (say 10s) ?
@maximedenes That is a big demand.  I am running `native_compute` as the identity function on terms here, and the real time is 100x the user time.  So to make 10s user time, I'd have to wait 15 minutes for each individual call to `native_compute`.  Unless you're asking that I continue running it until the user-time hits 10s, which would take at least a couple of hours, and might stack-overflow on `nat`s being too big...
> Unless you're asking that I continue running it until the user-time hits 10s, which would take at least a couple of hours, and might stack-overflow on nats being too big...

Yeah no, I don't mean to torture you :) I was wondering if we could somehow observe timings that would be significant enough to eliminate GC cycles as a potential source of the problem.
Is there a tactic that invokes the GC?  If there is one (or if you hand me one), I can invoke that between every call to `native_compute`.
Or do you mean something else by "GC cycles"?
Note that https://github.com/coq/coq/issues/8092 is a big headache in trying to get accurate timing data here.  I jump through hoops to avoid every native_compiling identical terms in this example, because I'm not sure exactly where the cache shows up
