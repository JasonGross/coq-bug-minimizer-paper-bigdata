Comment author: @JasonGross

What is [set] doing that makes it so slow?


Fixpoint fact n := match n with 0 => 1 | S n' => n * fact n' end.

Definition foo := Eval compute in fact 8.
Goal fact 8 * 10 = foo * 10.
Proof.
  unfold foo.
  Time set (x := fact 8). (* Finished transaction in 14.363 secs (14.359u,0.004s) (successful) *)
  Time pose (fact 8) as x; change (fact 8) with x. (* Finished transaction in 0.169 secs (0.167u,0.s) (successful) *)


Comment author: @ppedrot

Looks like it's calling the unification, which attempts to perform a quadratic unfolding of the constant.


Comment author: @JasonGross

My understanding of [set] was that it worked on syntactic equality.  Is this not the case?  What's an example where it makes use of unification?


Comment author: @maximedenes

Well, you can give set a partial term (set X := f _), so syntactic equality is not enough. The inefficiency seems to come from the matching algorithm. In fact, with ssreflect's "set" tactic, your example goes through instantly (I just checked).

Bringing the matching algorithm of ssreflect to Coq is the goal of a pull request that is currently under discussion: https://github.com/coq/coq/pull/146

If the pull request is merged, we could plug Coq's "set" on this matching algorithm and fix your problem.


Comment author: @JasonGross

Interesting.  That seems like a good PR to merge.  What's blocking it?


Comment author: @herbelin

Yes, experimenting the compatibility of ssreflect's algorithm for set (and, ideally, provide at the same time an "in pattern" alternative to "at occurrences" to select occurrences, with a syntax to be invented) would be interesting, assuming that this is ok to others too. No objection from my side.

Otherwise, to answer Jason's question, I could not resist to look at why it is slow and the answer is that there are two causes for a time polynomial in the size of the goal expression. 

The first one is the occur_meta_or_undefined_evar test in unify_0 in unification.ml which is done on 40320, 40319, 40318, etc. Computing it in advance would solve this inefficiency with all uses of unify_0 taking advantage of it (including maybe ssreflect).

The second one is a call to closed0 in Find_subterm.replace_term_occ_gen_modulo (commit dbdff037af1 from Matthieu). I don't know if it was supposed to improve efficiency or to prevent incorrect solutions, but its effect on very large expression is bad.

In particular, as far as I could see, the matching strategy in itself is not concerned, and once these two causes of polynomial time are removed, one gets back a linear time.


Comment author: @JasonGross

Thanks for looking into this Hugo!

It would be nice to have this inefficiency solved in 8.5, and it sounds like the PR would be merged into trunk rather than 8.5?


Comment author: @herbelin

> It would be nice to have this inefficiency solved in 8.5, and it sounds like
> the PR would be merged into trunk rather than 8.5?

Unfortunately, I'm not so optimistic. I shall discuss with Matthieu about the find_subterm inefficiency which I suspect can be addressed. For the other source of inefficiency, my current diagnosis is that this would not go without either a change of semantics of set (*) or with a quick hack which would just complicate the unification algorithm further or with deeper changes in the unification algorithm which might have an impact on other features, so I would not do that for a bug-fix release.

(*) For instance, an easy possible change would be to renounce to a - in my eyes artificial and useless - feature of set which is that the search is specifically made modulo beta-iota on evar-free subterms as shown below:

Goal 1=0.
set (n := S ((fun x => 0) 0)).
(* gives n=0 *)

But then, even if the latter would look like an improvement to me, I'm reluctant to take such a decision in urgency just because of the inefficiency issue and it looks better to me to seize the opportunity of this report for having a true collective reflection for 8.6 or 8.7 on the subterm selection semantics we want for "set" (i.e. what exact amount of reasoning up to conversion we want) where the mathcomp extensive experience has something to say and where some ideas from ssreflect such as supporting matching on the type and selecting specific occurrences by contextual patterns could also be integrated.

So, in all cases, my analysis is that this would either require complicating in the very short term the unification algorithm what I'm reluctant at, even more that the inefficiency could probably be controlled user-side by avoiding manipulating too large expressions, or by a deeper collective reflection on the semantics of set we want and about the incompatibility price we are ready to pay for it.


